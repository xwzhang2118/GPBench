import os
import random
import torch
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import optuna
from scipy.stats import pearsonr

# å°è¯•å¯¼å…¥GPUåŠ é€Ÿç‰ˆæœ¬
try:
    import cudf
    import cupy as cp
    from cuml.ensemble import RandomForestRegressor as cuRandomForestRegressor
    CUML_AVAILABLE = True
    print("âœ“ RAPIDS cuML å¯ç”¨ï¼Œå°†æ”¯æŒ GPU åŠ é€Ÿ")
except ImportError:
    CUML_AVAILABLE = False
    print("âš  cuML ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ scikit-learn CPU ç‰ˆæœ¬")

# ä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯å¹¶è¿›è¡ŒRandomForestè®­ç»ƒ
def run_nested_cv_with_early_stopping(data, label, outer_cv, n_estimators, max_depth, use_gpu=True):
    best_corr_coefs = []
    best_maes = []
    best_r2s = []
    best_mses = []

    # æ£€æŸ¥GPUå¯ç”¨æ€§
    gpu_available = use_gpu and CUML_AVAILABLE and torch.cuda.is_available()
    
    if gpu_available:
        print("ğŸš€ ä½¿ç”¨ GPU åŠ é€Ÿéšæœºæ£®æ—")
    else:
        print("âš  ä½¿ç”¨ CPU ç‰ˆæœ¬ (scikit-learn)")

    import time
    time_star = time.time()
    
    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(data)):
        x_train, x_test = data[train_idx], data[test_idx]
        y_train, y_test = label[train_idx], label[test_idx]

        # # æ ‡å‡†åŒ–æ•°æ®
        # scaler = StandardScaler()
        # x_train = scaler.fit_transform(x_train)
        # x_test = scaler.transform(x_test)

        # # ==== y æ ‡å‡†åŒ– ====
        # scaler_y = StandardScaler()
        # y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).reshape(-1)
        # y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).reshape(-1)

        x_train = x_train.astype(np.float32)
        x_test = x_test.astype(np.float32)
        y_train_scaled = y_train.astype(np.float32)
        y_test_scaled = y_test.astype(np.float32)
        
        # å°†æ•°æ®è½¬æ¢ä¸º GPU æ ¼å¼
        x_train_gpu = cp.asarray(x_train)
        x_test_gpu = cp.asarray(x_test)
        y_train_gpu = cp.asarray(y_train_scaled)

        model = cuRandomForestRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            # min_samples_split=min_samples_split,
            # min_samples_leaf=min_samples_leaf,
            # max_features=max_features,
            random_state=42,
            n_streams=1  # ä½¿ç”¨å•ä¸ªæµä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
        )

        # è®­ç»ƒæ¨¡å‹
        model.fit(x_train_gpu, y_train_gpu)

        # é¢„æµ‹
        y_test_preds = model.predict(x_test_gpu)
        
        # å°†ç»“æœè½¬æ¢å› CPU
        y_test_preds = cp.asnumpy(y_test_preds)
        y_test_scaled_cpu = cp.asnumpy(cp.asarray(y_test_scaled))
        
        # # åæ ‡å‡†åŒ–
        # y_test_preds = scaler_y.inverse_transform(y_test_preds.reshape(-1, 1)).reshape(-1)
        # y_test_trues = scaler_y.inverse_transform(y_test_scaled_cpu.reshape(-1, 1)).reshape(-1)
        y_test_trues = y_test_scaled_cpu.reshape(-1)
        y_test_preds = y_test_preds.reshape(-1)

        # è®¡ç®—è¯„ä»·æŒ‡æ ‡
        corr_coef = np.corrcoef(y_test_preds, y_test_trues)[0, 1]
        mae = mean_absolute_error(y_test_trues, y_test_preds)
        mse = mean_squared_error(y_test_trues, y_test_preds)
        r2 = r2_score(y_test_trues, y_test_preds)

        best_corr_coefs.append(corr_coef)
        best_maes.append(mae)
        best_r2s.append(r2)
        best_mses.append(mse)

        acceleration_status = "GPU" if gpu_available else "CPU"
        print(f'Fold {fold + 1}[{acceleration_status}]: MAE={mae:.4f}, MSE={mse:.4f}, R2={r2:.4f}, Corr={corr_coef:.4f}')

    print("==== Final Results ====")
    acceleration_status = "GPU" if gpu_available else "CPU"
    print(f"åŠ é€Ÿæ–¹å¼: {acceleration_status}")
    print(f"MAE: {np.mean(best_maes):.4f} Â± {np.std(best_maes):.4f}")
    print(f"MSE: {np.mean(best_mses):.4f} Â± {np.std(best_mses):.4f}")
    print(f"R2 : {np.mean(best_r2s):.4f} Â± {np.std(best_r2s):.4f}")
    print(f"Corr: {np.mean(best_corr_coefs):.4f} Â± {np.std(best_corr_coefs):.4f}")

    print(f"Time: {time.time() - time_star:.2f}s")
    return np.mean(best_corr_coefs)

# è®¾ç½®éšæœºç§å­
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def main(data, label, use_gpu=True):
    set_seed(42)

    # ç›®æ ‡å‡½æ•°ï¼Œç”¨äºOptunaä¼˜åŒ–
    def objective(trial):
        n_estimators = trial.suggest_int("n_estimators", 100, 1000)
        max_depth = trial.suggest_int("max_depth", 3, 10)
        # min_samples_split = trial.suggest_int("min_samples_split", 2, 10)
        # min_samples_leaf = trial.suggest_int("min_samples_leaf", 1, 10)
        # max_features = trial.suggest_float("max_features", 0.1, 1)
        
        outer_cv = KFold(n_splits=10, shuffle=True, random_state=42)
        
        corr_score = run_nested_cv_with_early_stopping(
            data=data,
            label=label,
            outer_cv=outer_cv,
            n_estimators=n_estimators,
            max_depth=max_depth,
            # min_samples_split=min_samples_split,
            # min_samples_leaf=min_samples_leaf,
            # max_features=max_features,
            use_gpu=use_gpu
        )
        return corr_score

    # è¿è¡ŒOptunaè¶…å‚æ•°ä¼˜åŒ–
    study = optuna.create_study(direction="maximize")
    
    # æ·»åŠ GPUä¿¡æ¯åˆ°study
    study.set_user_attr('gpu_available', torch.cuda.is_available())
    study.set_user_attr('using_gpu', use_gpu and torch.cuda.is_available())
    
    study.optimize(objective, n_trials=20)

    print("æœ€ä½³å‚æ•°:", study.best_params)
    print(f"ä¼˜åŒ–å®Œæˆ - ä½¿ç”¨ {'GPU' if (use_gpu and torch.cuda.is_available()) else 'CPU'}")
    return study.best_params



if __name__ == '__main__':
    main()